#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëª¨ë“ˆí™”ëœ í¬ë¡¤ë§ ì‹œìŠ¤í…œ - ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
Intel Core i5-4210M í™˜ê²½ ìµœì í™”
"""

import os
import sys
import logging
import traceback
from datetime import datetime
from dotenv import load_dotenv

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ
from utils.system_analyzer import SystemAnalyzer
from utils.excel_processor import ExcelProcessor
from utils.data_mapper import DataMapper
from utils.crawling_engine import CrawlingEngine
from utils.verification_engine import VerificationEngine
from config.settings import get_optimal_config, display_system_config

def setup_logger():
    """ë¡œê¹… ì„¤ì •"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f'crawler_main_{timestamp}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger('MainCrawler')

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = setup_logger()
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        
        logger.info("ğŸš€ ëª¨ë“ˆí™”ëœ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘!")
        print("=" * 80)
        print("ğŸš€ ëª¨ë“ˆí™”ëœ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
        print("=" * 80)
        
        # 1. ì‹œìŠ¤í…œ ë¶„ì„ ë° ìµœì í™” ì„¤ì •
        logger.info("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ë¶„ì„ ì‹œì‘")
        system_analyzer = SystemAnalyzer(logger)
        
        # 2. ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬
        logger.info("ğŸ“Š ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        excel_processor = ExcelProcessor(logger)
        
        # íŒŒì¼ ê²½ë¡œ ì…ë ¥
        while True:
            file_path = input("\nğŸ“‚ ì²˜ë¦¬í•  ì—‘ì…€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: academy2.xlsx): ").strip()
            if not file_path:
                file_path = 'academy2.xlsx'
            
            if os.path.exists(file_path):
                break
            else:
                print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
        
        # ì—‘ì…€ íŒŒì¼ ë¡œë“œ
        if not excel_processor.load_excel_file(file_path):
            logger.error("âŒ ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # í—¤ë” í–‰ ê°ì§€
        header_row = excel_processor.detect_header_row()
        
        # AI ì´ˆê¸°í™”
        api_key = os.getenv('GEMINI_API_KEY')
        if api_key:
            excel_processor.initialize_ai(api_key)
        
        # í—¤ë” ë¶„ì„ ë° ë§¤í•‘
        logger.info("ğŸ¤– í—¤ë” ë¶„ì„ ë° ë§¤í•‘ ì‹œì‘")
        
        # AI í—¤ë” ë¶„ì„ ì‹œë„
        header_mapping = excel_processor.analyze_headers_with_ai()
        
        # AI ë¶„ì„ ê²°ê³¼ í™•ì¸
        if not header_mapping:
            logger.warning("âš ï¸ AI í—¤ë” ë¶„ì„ ì‹¤íŒ¨, ìˆ˜ë™ ë§¤í•‘ í•„ìš”")
            header_mapping = excel_processor.manual_header_mapping()
        else:
            # ì‚¬ìš©ì í™•ì¸
            print("\nğŸ¤– AI í—¤ë” ë¶„ì„ ê²°ê³¼:")
            for standard_col, original_header in header_mapping.items():
                standard_name = excel_processor.standard_columns.get(standard_col, standard_col)
                print(f"   {standard_name} â† {original_header}")
            
            confirm = input("\nì´ ë§¤í•‘ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if confirm != 'y':
                header_mapping = excel_processor.manual_header_mapping()
        
        # í—¤ë” ë§¤í•‘ ì ìš©
        if not excel_processor.apply_header_mapping(header_mapping):
            logger.error("âŒ í—¤ë” ë§¤í•‘ ì ìš© ì‹¤íŒ¨")
            return
        
        # 3. ë°ì´í„° ì •ì œ
        logger.info("ğŸ—‚ï¸  ë°ì´í„° ì •ì œ ì‹œì‘")
        data_mapper = DataMapper(logger)
        
        processed_data = excel_processor.get_processed_data()
        if not data_mapper.load_data(processed_data):
            logger.error("âŒ ë°ì´í„° ë§¤í¼ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # ì „ì²´ ë°ì´í„° ì •ì œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        if not data_mapper.process_all():
            logger.error("âŒ ë°ì´í„° ì •ì œ ì‹¤íŒ¨")
            return
        
        # ì •ì œëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        cleaned_data = data_mapper.get_processed_data()
        
        # 4. í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™”
        logger.info("ğŸš€ í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™”")
        crawling_engine = CrawlingEngine(logger)
        
        # 5. ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
        print("\nğŸ“‹ ì²˜ë¦¬ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì „ì²´ ë°ì´í„° ì²˜ë¦¬ (ê¶Œì¥)")
        print("2. ì§€ì—­ë³„ ê°œë³„ ì²˜ë¦¬")
        print("3. í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì²« 10ê°œ ë°ì´í„°ë§Œ)")
        
        while True:
            choice = input("ì„ íƒ (1-3): ").strip()
            if choice in ['1', '2', '3']:
                break
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1, 2, 3 ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        
        # 6. í¬ë¡¤ë§ ì‹¤í–‰
        all_results = []
        
        if choice == '1':
            # ì „ì²´ ë°ì´í„° ì²˜ë¦¬
            logger.info("ğŸ”„ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
            institutions = cleaned_data.to_dict('records')
            results = crawling_engine.process_institution_batch(
                institutions, crawling_engine.process_institution_parallel
            )
            all_results.extend(results)
            
        elif choice == '2':
            # ì§€ì—­ë³„ ê°œë³„ ì²˜ë¦¬
            logger.info("ğŸ—ºï¸  ì§€ì—­ë³„ ê°œë³„ ì²˜ë¦¬ ì‹œì‘")
            from utils.constants import REGIONS
            
            for region in REGIONS:
                region_data = cleaned_data[cleaned_data['region'] == region]
                if not region_data.empty:
                    logger.info(f"ğŸ”„ {region} ì§€ì—­ ì²˜ë¦¬ ì‹œì‘")
                    results = crawling_engine.process_region_data(region_data, region)
                    all_results.extend(results)
                    
                    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                    if results:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"results_{region}_{timestamp}.xlsx"
                        crawling_engine.save_results(results, filename)
                else:
                    logger.info(f"âš ï¸ {region} ì§€ì—­ ë°ì´í„° ì—†ìŒ")
            
        elif choice == '3':
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘")
            test_data = cleaned_data.head(10)
            institutions = test_data.to_dict('records')
            results = crawling_engine.process_institution_batch(
                institutions, crawling_engine.process_institution_parallel
            )
            all_results.extend(results)
        
        # 7. ìµœì¢… ë°ì´í„° ì •ì œ ë° ê²€ì¦
        if all_results:
            logger.info("ğŸ” ìµœì¢… ë°ì´í„° ì •ì œ ë° ê²€ì¦")
            
            # ìµœì¢… ë°ì´í„° ì •ì œ
            final_mapper = DataMapper(logger)
            import pandas as pd
            final_df = pd.DataFrame(all_results)
            
            final_mapper.load_data(final_df)
            final_mapper.process_all()
            
            final_data = final_mapper.get_processed_data()
            final_results = final_data.to_dict('records')
            
            # 8. ìµœì¢… ê²°ê³¼ ì €ì¥
            logger.info("ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            final_filename = f"final_crawling_results_{timestamp}.xlsx"
            saved_path = crawling_engine.save_results(final_results, final_filename)
            
            # 9. í†µê³„ ë° ìš”ì•½
            logger.info("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
            stats = crawling_engine.get_crawling_stats()
            cleanup_stats = final_mapper.get_cleanup_summary()
            
            print("\n" + "=" * 80)
            print("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
            print("=" * 80)
            print(f"ì „ì²´ ì²˜ë¦¬ ê¸°ê´€: {stats['total_institutions']:,}ê°œ")
            print(f"ì„±ê³µ ì¶”ì¶œ: {stats['successful_extractions']:,}ê°œ")
            print(f"ì‹¤íŒ¨ ì¶”ì¶œ: {stats['failed_extractions']:,}ê°œ")
            print(f"ê²€ì¦ ì™„ë£Œ: {stats['verified_contacts']:,}ê°œ")
            
            if stats['start_time'] and stats['end_time']:
                elapsed = stats['end_time'] - stats['start_time']
                print(f"ì´ ì†Œìš”ì‹œê°„: {elapsed}")
                
                if stats['processed_institutions'] > 0:
                    success_rate = (stats['successful_extractions'] / stats['processed_institutions']) * 100
                    print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
            
            print(f"\nğŸ’¾ ìµœì¢… ê²°ê³¼ íŒŒì¼: {saved_path}")
            print("=" * 80)
            
        else:
            logger.warning("âš ï¸ ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        
    finally:
        # ì •ë¦¬ ì‘ì—…
        logger.info("ğŸ§¹ ì •ë¦¬ ì‘ì—… ì‹œì‘")
        
        try:
            if 'system_analyzer' in locals():
                system_analyzer.cleanup()
            if 'crawling_engine' in locals():
                crawling_engine.cleanup()
        except Exception as e:
            logger.error(f"âŒ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")
        
        logger.info("âœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")

if __name__ == "__main__":
    main() 