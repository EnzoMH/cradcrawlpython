#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ì—”ì§„ í´ë˜ìŠ¤
"""

import time
import random
import logging
import re
from typing import Optional, Dict, List
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import requests

class HomepageCrawler:
    """í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, logger=None):
        """
        í™ˆí˜ì´ì§€ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            logger: ë¡œê¹… ê°ì²´ (ê¸°ë³¸ê°’: None)
        """
        self.logger = logger or logging.getLogger(__name__)
        
        # íŒ©ìŠ¤ë²ˆí˜¸ ì •ê·œì‹ íŒ¨í„´
        self.fax_patterns = [
            r'íŒ©ìŠ¤[\s:ï¼š]*(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
            r'fax[\s:ï¼š]*(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
            r'F[\s:ï¼š]*(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
            r'ì „ì†¡[\s:ï¼š]*(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
            r'(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4}).*íŒ©ìŠ¤',
            r'(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4}).*fax',
            r'(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4}).*ì „ì†¡',
        ]
        
        # ì „í™”ë²ˆí˜¸ ì •ê·œì‹ íŒ¨í„´
        self.phone_patterns = [
            r'ì „í™”[\s:ï¼š]*(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
            r'tel[\s:ï¼š]*(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
            r'ì—°ë½ì²˜[\s:ï¼š]*(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
            r'(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})',
        ]
        
        # ì œì™¸í•  íŒ¨í„´ë“¤
        self.exclude_patterns = [
            r'^\d{4}$',  # 4ìë¦¬ ìˆ«ì
            r'^\d{1,3}$',  # 1-3ìë¦¬ ìˆ«ì
            r'^\d{4}-\d{2}-\d{2}$',  # ë‚ ì§œ í˜•ì‹
            r'^\d{6}$',  # 6ìë¦¬ ìˆ«ì
        ]
        
        # ìš”ì²­ í—¤ë”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def crawl_homepage(self, url: str, use_selenium: bool = False, driver=None) -> Optional[Dict]:
        """
        í™ˆí˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            url: í¬ë¡¤ë§í•  URL
            use_selenium: Selenium ì‚¬ìš© ì—¬ë¶€
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤ (Selenium ì‚¬ìš©ì‹œ)
            
        Returns:
            Optional[Dict]: í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„°
        """
        try:
            if use_selenium and driver:
                return self._crawl_with_selenium(url, driver)
            else:
                return self._crawl_with_requests(url)
                
        except Exception as e:
            self.logger.error(f"âŒ í™ˆí˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def _crawl_with_selenium(self, url: str, driver) -> Optional[Dict]:
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        try:
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)
            time.sleep(random.uniform(2, 4))
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source
            
            # í˜ì´ì§€ ì œëª©
            title = driver.title
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            body_text = ""
            try:
                body_element = driver.find_element(By.TAG_NAME, "body")
                body_text = body_element.text
            except:
                pass
            
            result = {
                'url': url,
                'title': title,
                'html_content': page_source,
                'text_content': body_text,
                'method': 'selenium'
            }
            
            self.logger.info(f"ğŸŒ Selenium í¬ë¡¤ë§ ì™„ë£Œ: {url}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def _crawl_with_requests(self, url: str) -> Optional[Dict]:
        """Requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        try:
            # HTTP ìš”ì²­
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ì„¤ì •
            response.encoding = response.apparent_encoding
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = soup.title.string if soup.title else ""
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            text_content = soup.get_text()
            
            result = {
                'url': url,
                'title': title.strip() if title else "",
                'html_content': response.text,
                'text_content': text_content,
                'method': 'requests'
            }
            
            self.logger.info(f"ğŸŒ Requests í¬ë¡¤ë§ ì™„ë£Œ: {url}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Requests í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def extract_fax_from_html(self, html_content: str) -> List[str]:
        """HTML ë‚´ìš©ì—ì„œ íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            if not html_content:
                return []
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for script in soup(["script", "style"]):
                script.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = soup.get_text()
            
            return self._extract_fax_from_text(text_content)
            
        except Exception as e:
            self.logger.error(f"âŒ HTML íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_phone_from_html(self, html_content: str) -> List[str]:
        """HTML ë‚´ìš©ì—ì„œ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            if not html_content:
                return []
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for script in soup(["script", "style"]):
                script.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = soup.get_text()
            
            return self._extract_phone_from_text(text_content)
            
        except Exception as e:
            self.logger.error(f"âŒ HTML ì „í™”ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_fax_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            if not text:
                return []
            
            fax_numbers = []
            
            for pattern in self.fax_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        fax_number = match[0] if match[0] else match[1] if len(match) > 1 else ""
                    else:
                        fax_number = match
                    
                    if fax_number and self._is_valid_number(fax_number):
                        fax_numbers.append(fax_number)
            
            # ì¤‘ë³µ ì œê±°
            return list(set(fax_numbers))
            
        except Exception as e:
            self.logger.error(f"âŒ í…ìŠ¤íŠ¸ íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_phone_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            if not text:
                return []
            
            phone_numbers = []
            
            for pattern in self.phone_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        phone_number = match[0] if match[0] else match[1] if len(match) > 1 else ""
                    else:
                        phone_number = match
                    
                    if phone_number and self._is_valid_number(phone_number):
                        phone_numbers.append(phone_number)
            
            # ì¤‘ë³µ ì œê±°
            return list(set(phone_numbers))
            
        except Exception as e:
            self.logger.error(f"âŒ í…ìŠ¤íŠ¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _is_valid_number(self, number: str) -> bool:
        """ë²ˆí˜¸ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # ìˆ«ìë§Œ ì¶”ì¶œ
            digits = re.sub(r'[^\d]', '', number)
            
            # ê¸¸ì´ ì²´í¬
            if len(digits) < 8 or len(digits) > 11:
                return False
            
            # ì œì™¸ íŒ¨í„´ ì²´í¬
            for pattern in self.exclude_patterns:
                if re.match(pattern, digits):
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ë²ˆí˜¸ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {number} - {e}")
            return False
    
    def extract_contact_info(self, page_data: Dict) -> Dict:
        """í˜ì´ì§€ ë°ì´í„°ì—ì„œ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            result = {
                'fax_numbers': [],
                'phone_numbers': [],
                'email_addresses': [],
                'addresses': []
            }
            
            if not page_data:
                return result
            
            # HTML ë‚´ìš©ì—ì„œ ì¶”ì¶œ
            if 'html_content' in page_data:
                result['fax_numbers'] = self.extract_fax_from_html(page_data['html_content'])
                result['phone_numbers'] = self.extract_phone_from_html(page_data['html_content'])
                result['email_addresses'] = self._extract_emails_from_html(page_data['html_content'])
                result['addresses'] = self._extract_addresses_from_html(page_data['html_content'])
            
            # í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œ ì¶”ê°€ ì¶”ì¶œ
            if 'text_content' in page_data:
                text_fax = self._extract_fax_from_text(page_data['text_content'])
                text_phone = self._extract_phone_from_text(page_data['text_content'])
                
                # ì¤‘ë³µ ì œê±°í•˜ë©° ë³‘í•©
                result['fax_numbers'] = list(set(result['fax_numbers'] + text_fax))
                result['phone_numbers'] = list(set(result['phone_numbers'] + text_phone))
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'fax_numbers': [],
                'phone_numbers': [],
                'email_addresses': [],
                'addresses': []
            }
    
    def _extract_emails_from_html(self, html_content: str) -> List[str]:
        """HTMLì—ì„œ ì´ë©”ì¼ ì£¼ì†Œ ì¶”ì¶œ"""
        try:
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            emails = re.findall(email_pattern, html_content)
            return list(set(emails))
        except Exception as e:
            self.logger.error(f"âŒ ì´ë©”ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_addresses_from_html(self, html_content: str) -> List[str]:
        """HTMLì—ì„œ ì£¼ì†Œ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ì£¼ì†Œ íŒ¨í„´ (ì‹œ/ë„ + ì‹œ/êµ°/êµ¬ í¬í•¨)
            address_patterns = [
                r'[ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼].*?[ì‹œ|êµ°|êµ¬].*?[ë™|ì|ë©´]',
                r'\d{5}.*?[ì‹œ|êµ°|êµ¬].*?[ë™|ì|ë©´]',
            ]
            
            addresses = []
            soup = BeautifulSoup(html_content, 'html.parser')
            text_content = soup.get_text()
            
            for pattern in address_patterns:
                matches = re.findall(pattern, text_content)
                addresses.extend(matches)
            
            return list(set(addresses))
            
        except Exception as e:
            self.logger.error(f"âŒ ì£¼ì†Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_page_summary(self, page_data: Dict) -> Dict:
        """í˜ì´ì§€ ìš”ì•½ ì •ë³´ ìƒì„±"""
        try:
            if not page_data:
                return {}
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´
            text_length = len(page_data.get('text_content', ''))
            
            # HTML ê¸¸ì´
            html_length = len(page_data.get('html_content', ''))
            
            # ì—°ë½ì²˜ ì •ë³´
            contact_info = self.extract_contact_info(page_data)
            
            return {
                'url': page_data.get('url', ''),
                'title': page_data.get('title', ''),
                'text_length': text_length,
                'html_length': html_length,
                'fax_count': len(contact_info['fax_numbers']),
                'phone_count': len(contact_info['phone_numbers']),
                'email_count': len(contact_info['email_addresses']),
                'address_count': len(contact_info['addresses']),
                'method': page_data.get('method', 'unknown')
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í˜ì´ì§€ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}


# ì „ì—­ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´)
def crawl_homepage_simple(url: str):
    """ê°„ë‹¨í•œ í™ˆí˜ì´ì§€ í¬ë¡¤ë§ (í˜¸í™˜ì„± í•¨ìˆ˜)"""
    crawler = HomepageCrawler()
    return crawler.crawl_homepage(url)

def extract_fax_from_html_simple(html_content: str):
    """ê°„ë‹¨í•œ HTML íŒ©ìŠ¤ë²ˆí˜¸ ì¶”ì¶œ (í˜¸í™˜ì„± í•¨ìˆ˜)"""
    crawler = HomepageCrawler()
    return crawler.extract_fax_from_html(html_content) 