#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìµœì¢… ì„¼í„° í¬ë¡¤ë§ ì‹œìŠ¤í…œ - auction1.co.kr

ë°ì´í„° êµ¬ì¡°:
- URL: https://www.auction1.co.kr/etc_service/dong_office.php?sido=11&gugun=680
- ì„¼í„° ì •ë³´: <div class='content_addr_list'> ì•ˆì— í¬í•¨
- ì „í™”ë²ˆí˜¸: TEL : <span class='brown'>02-3423-7670</span>
- íŒ©ìŠ¤: FAX : <span class='brown'>02-3423-8954</span>
- í™ˆí˜ì´ì§€: <a href='URL'>í™ˆí˜ì´ì§€</a>
- ì£¼ì†Œ: ì†Œì¬ì§€ : <span class='black'>ì£¼ì†Œ</span>
"""

import os
import time
import random
import requests
import pandas as pd
from bs4 import BeautifulSoup
import urllib3
import re
from datetime import datetime
from typing import List, Dict, Optional

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class CenterCrawler:
    """ì„¼í„° ì •ë³´ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_url = "https://www.auction1.co.kr"
        self.endpoint = "/etc_service/dong_office.php"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
    def load_region_data(self, file_path: str) -> pd.DataFrame:
        """ì§€ì—­ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ ì§€ì—­ ë°ì´í„° ë¡œë“œ: {file_path}")
        df = pd.read_excel(file_path)
        print(f"âœ… {len(df)}ê°œ ì§€ì—­ ë¡œë“œ ì™„ë£Œ")
        return df
    
    def extract_center_info(self, html_content: str, sido: str, gugun: str) -> List[Dict]:
        """HTMLì—ì„œ ì„¼í„° ì •ë³´ ì¶”ì¶œ"""
        centers = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # content_addr_list í´ë˜ìŠ¤ë¥¼ ê°€ì§„ div ì°¾ê¸°
            center_divs = soup.find_all('div', class_='content_addr_list')
            
            print(f"  ğŸ“‹ ë°œê²¬ëœ ì„¼í„°: {len(center_divs)}ê°œ")
            
            for div in center_divs:
                center_info = {
                    'sido': sido,
                    'gugun': gugun,
                    'center_name': '',
                    'phone': '',
                    'fax': '',
                    'homepage': '',
                    'address': '',
                    'postal_code': ''
                }
                
                # ì„¼í„°ëª… ì¶”ì¶œ
                title_span = div.find('span', class_='content_addr_title')
                if title_span:
                    center_info['center_name'] = title_span.get_text(strip=True)
                
                # í™ˆí˜ì´ì§€ ë§í¬ ì¶”ì¶œ - ëª¨ë“  ë§í¬ ì¤‘ì—ì„œ 'í™ˆí˜ì´ì§€' í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ê²ƒ ì°¾ê¸°
                links = div.find_all('a', href=True)
                for link in links:
                    link_text = link.get_text(strip=True)
                    if 'í™ˆí˜ì´ì§€' in link_text:
                        center_info['homepage'] = link.get('href')
                        break
                
                # ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
                ul_tag = div.find('ul')
                if ul_tag:
                    li_tags = ul_tag.find_all('li')
                    
                    for li in li_tags:
                        li_text = li.get_text(strip=True)
                        
                        # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ - TELì´ í¬í•¨ëœ liì—ì„œ brown í´ë˜ìŠ¤ span ì°¾ê¸°
                        if 'TEL' in li_text and 'FAX' not in li_text:
                            phone_span = li.find('span', class_='brown')
                            if phone_span:
                                center_info['phone'] = phone_span.get_text(strip=True)
                        
                        # íŒ©ìŠ¤ ì¶”ì¶œ - FAXê°€ í¬í•¨ëœ liì—ì„œ brown í´ë˜ìŠ¤ span ì°¾ê¸°
                        elif 'FAX' in li_text:
                            fax_span = li.find('span', class_='brown')
                            if fax_span:
                                center_info['fax'] = fax_span.get_text(strip=True)
                        
                        # ì£¼ì†Œ ì¶”ì¶œ - ì†Œì¬ì§€ê°€ í¬í•¨ëœ liì—ì„œ black í´ë˜ìŠ¤ span ì°¾ê¸°
                        elif 'ì†Œì¬ì§€' in li_text or 'ì£¼ì†Œ' in li_text:
                            addr_span = li.find('span', class_='black')
                            if addr_span:
                                addr_text = addr_span.get_text(strip=True)
                                center_info['address'] = addr_text
                                
                                # ìš°í¸ë²ˆí˜¸ ì¶”ì¶œ
                                postal_match = re.search(r'ìš°:\s*(\d{5})', li_text)
                                if postal_match:
                                    center_info['postal_code'] = postal_match.group(1)
                
                # ìµœì†Œí•œ ì„¼í„°ëª…ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€ (ì „í™”ë²ˆí˜¸ë‚˜ íŒ©ìŠ¤ë²ˆí˜¸ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì¢‹ìŒ)
                if center_info['center_name']:
                    centers.append(center_info)
                    fax_info = f", FAX: {center_info['fax']}" if center_info['fax'] else ""
                    homepage_info = f", í™ˆí˜ì´ì§€: âœ“" if center_info['homepage'] else ""
                    print(f"    âœ… {center_info['center_name']}: TEL: {center_info['phone']}{fax_info}{homepage_info}")
                
        except Exception as e:
            print(f"  âŒ ì„¼í„° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return centers
    
    def crawl_region(self, sido: str, gugun: str, sido_code: str, gugun_code: str) -> List[Dict]:
        """íŠ¹ì • ì§€ì—­ í¬ë¡¤ë§"""
        try:
            url = f"{self.base_url}{self.endpoint}?sido={sido_code}&gugun={gugun_code}"
            print(f"ğŸ” í¬ë¡¤ë§: {sido} {gugun}")
            print(f"ğŸŒ URL: {url}")
            
            response = self.session.get(url, timeout=15, verify=False)
            
            if response.status_code == 200:
                centers = self.extract_center_info(response.text, sido, gugun)
                print(f"âœ… {sido} {gugun}: {len(centers)}ê°œ ì„¼í„° ìˆ˜ì§‘")
                return centers
            else:
                print(f"âŒ {sido} {gugun}: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ {sido} {gugun} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def crawl_all_regions(self, region_df: pd.DataFrame) -> pd.DataFrame:
        """ì „ì²´ ì§€ì—­ í¬ë¡¤ë§"""
        print(f"ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘: {len(region_df)}ê°œ ì§€ì—­")
        
        all_centers = []
        
        for idx, row in region_df.iterrows():
            sido = row['ì‹œë„']
            gugun = row['êµ°êµ¬']
            
            # URLì—ì„œ sido, gugun ì½”ë“œ ì¶”ì¶œ
            url_path = row['ì£¼ì†Œ.1']  # /dong_office.php?sido=11&gugun=680
            
            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì½”ë“œ ì¶”ì¶œ
            sido_match = re.search(r'sido=(\d+)', url_path)
            gugun_match = re.search(r'gugun=(\d+)', url_path)
            
            if sido_match and gugun_match:
                sido_code = sido_match.group(1)
                gugun_code = gugun_match.group(1)
                
                print(f"\nğŸ“ ì§„í–‰ìƒí™©: {idx+1}/{len(region_df)}")
                centers = self.crawl_region(sido, gugun, sido_code, gugun_code)
                all_centers.extend(centers)
                
                # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(random.uniform(1.0, 2.0))
                
                # ì¤‘ê°„ ì €ì¥ (10ê°œ ì§€ì—­ë§ˆë‹¤)
                if (idx + 1) % 10 == 0:
                    self._save_intermediate_results(all_centers, idx + 1)
            else:
                print(f"âš ï¸ {sido} {gugun}: URL ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        
        # ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±
        if all_centers:
            result_df = pd.DataFrame(all_centers)
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_centers)}ê°œ ì„¼í„°")
            return result_df
        else:
            print("\nâš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()
    
    def _save_intermediate_results(self, centers: List[Dict], progress: int):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        try:
            if centers:
                temp_df = pd.DataFrame(centers)
                temp_filename = f"center_progress_{progress}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                
                # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
                column_order = ['sido', 'gugun', 'center_name', 'phone', 'fax', 'homepage', 'address', 'postal_code']
                existing_columns = [col for col in column_order if col in temp_df.columns]
                temp_df_ordered = temp_df[existing_columns]
                
                # Excel ì €ì¥ (ì—¬ëŸ¬ ì—”ì§„ ëŒ€ì‘)
                try:
                    with pd.ExcelWriter(temp_filename, engine='openpyxl') as writer:
                        temp_df_ordered.to_excel(writer, index=False, sheet_name='ì„¼í„°ì •ë³´')
                except ImportError:
                    try:
                        with pd.ExcelWriter(temp_filename, engine='xlsxwriter') as writer:
                            temp_df_ordered.to_excel(writer, index=False, sheet_name='ì„¼í„°ì •ë³´')
                    except ImportError:
                        temp_df_ordered.to_excel(temp_filename, index=False)
                
                print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {temp_filename} ({len(centers)}ê°œ)")
        except Exception as e:
            print(f"âš ï¸ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def save_results(self, result_df: pd.DataFrame, filename: str = None):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        if filename is None:
            filename = f"center_crawling_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        try:
            # ë¹ˆ DataFrame ì²´í¬
            if result_df.empty:
                print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬ (ë³´ê¸° ì¢‹ê²Œ)
            column_order = ['sido', 'gugun', 'center_name', 'phone', 'fax', 'homepage', 'address', 'postal_code']
            existing_columns = [col for col in column_order if col in result_df.columns]
            result_df_ordered = result_df[existing_columns]
            
            # Excel ì €ì¥ (openpyxl ì—†ìœ¼ë©´ xlsxwriter ì‚¬ìš©)
            try:
                with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                    result_df_ordered.to_excel(writer, index=False, sheet_name='ì„¼í„°ì •ë³´')
            except ImportError:
                print("ğŸ“ openpyxlì´ ì—†ì–´ì„œ xlsxwriterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                try:
                    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
                        result_df_ordered.to_excel(writer, index=False, sheet_name='ì„¼í„°ì •ë³´')
                except ImportError:
                    print("ğŸ“ xlsxwriterë„ ì—†ì–´ì„œ ê¸°ë³¸ ì—”ì§„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    result_df_ordered.to_excel(filename, index=False)
            
            print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥: {filename}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(filename)
            print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)")
            
            # ìš”ì•½ ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
            print(f"  - ì´ ì„¼í„° ìˆ˜: {len(result_df)}ê°œ")
            
            if len(result_df) > 0:
                print(f"  - ì‹œë„ë³„ ë¶„í¬:")
                sido_counts = result_df['sido'].value_counts().head(10)
                for sido, count in sido_counts.items():
                    print(f"    {sido}: {count}ê°œ")
                
                # ë°ì´í„° ì™„ì„±ë„ í†µê³„
                total_centers = len(result_df)
                phone_count = len(result_df[result_df['phone'].notna() & (result_df['phone'] != '')])
                fax_count = len(result_df[result_df['fax'].notna() & (result_df['fax'] != '')])
                homepage_count = len(result_df[result_df['homepage'].notna() & (result_df['homepage'] != '')])
                address_count = len(result_df[result_df['address'].notna() & (result_df['address'] != '')])
                
                print(f"\n  ğŸ“ ë°ì´í„° ì™„ì„±ë„:")
                print(f"    ì „í™”ë²ˆí˜¸: {phone_count}ê°œ ({phone_count/total_centers*100:.1f}%)")
                print(f"    íŒ©ìŠ¤ë²ˆí˜¸: {fax_count}ê°œ ({fax_count/total_centers*100:.1f}%)")
                print(f"    í™ˆí˜ì´ì§€: {homepage_count}ê°œ ({homepage_count/total_centers*100:.1f}%)")
                print(f"    ì£¼ì†Œì •ë³´: {address_count}ê°œ ({address_count/total_centers*100:.1f}%)")
            
            return True
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ¢ auction1.co.kr ì„¼í„° ì •ë³´ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    try:
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = CenterCrawler()
        
        # ì§€ì—­ ë°ì´í„° ë¡œë“œ
        region_file = "rawdatafile/ì„¼í„°í¬ë¡¤ë§ì •ë³´.xlsx"
        region_df = crawler.load_region_data(region_file)
        
        print(f"\nğŸ“Š ì‹œë„ë³„ ì§€ì—­ ìˆ˜:")
        print(region_df['ì‹œë„'].value_counts())
        
        # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‹¤í–‰
        print(f"\nğŸ” ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (ì„œìš¸ ê°•ë‚¨êµ¬)...")
        sample_row = region_df.iloc[0]
        
        # URLì—ì„œ ì½”ë“œ ì¶”ì¶œ
        url_path = sample_row['ì£¼ì†Œ.1']
        sido_match = re.search(r'sido=(\d+)', url_path)
        gugun_match = re.search(r'gugun=(\d+)', url_path)
        
        if sido_match and gugun_match:
            sample_centers = crawler.crawl_region(
                sample_row['ì‹œë„'], 
                sample_row['êµ°êµ¬'],
                sido_match.group(1),
                gugun_match.group(1)
            )
            
            if sample_centers:
                print("âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
                print("ìƒ˜í”Œ ë°ì´í„°:")
                for center in sample_centers[:2]:
                    print(f"  - {center['center_name']}: {center['phone']}")
                
                # ì‚¬ìš©ì í™•ì¸
                choice = input("\nì „ì²´ í¬ë¡¤ë§ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
                
                if choice == 'y':
                    # ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
                    result_df = crawler.crawl_all_regions(region_df)
                    
                    if not result_df.empty:
                        # ê²°ê³¼ ì €ì¥
                        save_success = crawler.save_results(result_df)
                        if save_success:
                            print(f"\nğŸ‰ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")
                        else:
                            print(f"\nâš ï¸ í¬ë¡¤ë§ì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        print(f"\nâš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print("í¬ë¡¤ë§ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        else:
            print("âŒ URL ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 